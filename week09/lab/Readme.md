# Lab 9 - Distributed training



These labs will be focused on aspects of multi-gpu and multi-node model training.
The exercises would involve training diverse networks (NLP, CV) with a focus on data parallelism.
We may also explore model parallelism.

The current ideas would be to outline how this is done using PyTorch Lightning, on NLP and object detection tasks
we will just cover simple examples here and explain how to it works
